{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of 如何使用自有資料集訓練在 Colab 上訓練 YOLO 物件辨識.ipynb","provenance":[{"file_id":"1EA0x98UdBqCzbW25aPUkxZkZPMRJHlsz","timestamp":1592735484951}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5I_FXY-bftgC","colab_type":"text"},"source":["\n","本程式碼請搭配文章「[在 Colab 上利用 Yolov3 框架和自有標住資料來訓練自己的物件辨識系統](http://bit.ly/2OAfpq0)」 一起服用。\n","\n","其實原意是利用圖片辨識手遊的畫面，來幫我自動玩手遊練功。在找資料的時候，很多人都推薦 Yolov3，優點是非常快速，剛好利用在畫面變動巨大的手遊上。但使用遊戲畫面，可能只有我自己看得懂。所以這篇文章我用最普遍的貓和狗當作範例。能訓練貓和狗就能訓練自己其他的資料了。\n","\n","# 這篇文章會教你\n","在此範例中，我們將會訓練一個能偵測圖片中貓和狗的模型。你可以自由替換資料集，偵測自訂的物體。\n","\n","除此之外，還有：\n","* 利用 Colab 128G RAM GPU 來訓練你的 Yolo3 模型\n","* 掛載 Google Drive 檔案到 Colab 檔案系統中\n","* 將 PASCAL VOC 標籤格式轉換成 Yolo 用的標籤格式\n","* 產生 Yolo 訓練需要的 cfg 設定檔案\n","* 將訓練後的 weight 檔案同步至 Google Drive 中，避免遺失\n","* 如何利用 weight 檔案來辨識圖片中的內容\n","\n","# 事前準備\n","\n","在開始前，你需要先編譯好 Darknet 執行檔案。你可以參考我的另外一篇文章「[如何在 Colab 安裝 Darknet 框架訓練 YOLO v3 物件辨識並且最佳化 Colab 的訓練流程](http://bit.ly/33XjcEu)」，文章中會將編譯好的 Darknet 執行檔案放到 Google Drive目錄下。本文章將會利用這個執行檔案進行訓練。"]},{"cell_type":"markdown","metadata":{"id":"mEXkY-lagdBF","colab_type":"text"},"source":["\n","\n","\n","---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NAjezpOEilTY","colab_type":"text"},"source":["## 將 Google Drive 掛載到 Colab 目錄下\n","\n","掛載 Google Drive 的好處是不用每次都手動上傳或下載檔案，而且還能讓訓練好的模型檔案自動保存到 Google Drive。這樣就不會因為 Colab 中斷後就必須從頭訓練。"]},{"cell_type":"code","metadata":{"id":"wnqbZSdu0jjk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1592747129849,"user_tz":-480,"elapsed":34507,"user":{"displayName":"Gwo-Chuan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivghmngODeLxLuKZvFG4vSDW8-dveLXUDjGG11=s64","userId":"14590907014150515187"}},"outputId":"34794e66-8997-4181-da1e-7ca6456ec47f"},"source":["# 將 Google Drive 掛載到 Colab 目錄下\n","from google.colab import drive\n","drive.mount('/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U96J-wAdxQY_","colab_type":"text"},"source":["# 準備資料集\n","\n","您可以使用自己的資料夾。為了示範，這邊使用 [The Oxford-IIIT Pet Dataset](http://www.robots.ox.ac.uk/~vgg/data/pets/) 已經標注好的寵物資料來當作範例。我們會用到裡面的圖片檔以及 `annotations` 中的 `xml` 標註資料。"]},{"cell_type":"code","metadata":{"id":"qBVY0kfGxCoP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":421},"executionInfo":{"status":"ok","timestamp":1592747175986,"user_tz":-480,"elapsed":80617,"user":{"displayName":"Gwo-Chuan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivghmngODeLxLuKZvFG4vSDW8-dveLXUDjGG11=s64","userId":"14590907014150515187"}},"outputId":"a3a4ae36-422b-4039-dad3-082af1336410"},"source":["%cd /content\n","\n","!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n","!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n","\n","# 如果你已經把檔案下載到你的 Google Drive 就可以直接用複製的，速度會快一些\n","# !cp /drive/My\\ Drive/ColabDrive/The_Oxford_IIIT_Pet_Dataset/images.tar.gz /content/\n","# !cp /drive/My\\ Drive/ColabDrive/The_Oxford_IIIT_Pet_Dataset/annotations.tar.gz /content/\n","\n","# move image and label folder into pet_detection folder\n","!mkdir /content/pet_detection\n","!tar -xf images.tar.gz -C /content/pet_detection\n","!tar -xf annotations.tar.gz\n","!mv annotations/xmls /content/pet_detection/labels\n","!rm -fr annotations"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content\n","--2020-06-21 13:45:31--  http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n","Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n","Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 791918971 (755M) [application/x-gzip]\n","Saving to: ‘images.tar.gz’\n","\n","images.tar.gz       100%[===================>] 755.23M  28.8MB/s    in 27s     \n","\n","2020-06-21 13:45:59 (28.1 MB/s) - ‘images.tar.gz’ saved [791918971/791918971]\n","\n","--2020-06-21 13:46:00--  http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n","Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n","Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 19173078 (18M) [application/x-gzip]\n","Saving to: ‘annotations.tar.gz’\n","\n","annotations.tar.gz  100%[===================>]  18.28M  14.5MB/s    in 1.3s    \n","\n","2020-06-21 13:46:01 (14.5 MB/s) - ‘annotations.tar.gz’ saved [19173078/19173078]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"keKBT0IZi-gs","colab_type":"text"},"source":["## 定義後來會用到的檔案路徑\n","\n","路徑分為保存在 Colab 的，每次 Colab 被重置後就會消失。所以只放一些不重要的東西。\n","保存在 Google Drive 的檔案就是會被保存下來的，即使 Colab 被重置後，也不會消失；我們會把重要的東西存在這，例如設定檔、訓練到一半的模型等。"]},{"cell_type":"code","metadata":{"id":"vIBZ9vWTCkNo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592747175987,"user_tz":-480,"elapsed":80587,"user":{"displayName":"Gwo-Chuan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivghmngODeLxLuKZvFG4vSDW8-dveLXUDjGG11=s64","userId":"14590907014150515187"}}},"source":["# define constants\n","\n","LOCAL_IMAGES_DIR_PATH = \"/content/pet_detection/images\"\n","LOCAL_LABELS_DIR_PATH = \"/content/pet_detection/labels\"\n","LOCAL_YOLOS_DIR_PATH = \"/content/pet_detection/yolos\"\n","LOCAL_CFG_DIR_PATH = \"/content/pet_detection/cfg\"\n","GDRIVE_APP_BASE_DIR_REMOTE_PATH = \"/drive/My\\ Drive/train_yolo_with_custom_dataset_on_colab_101\"\n","\n","GDRIVE_APP_BASE_DIR_PATH = \"/content/app\"\n","GDRIVE_WEIGHTS_DIR_PATH = GDRIVE_APP_BASE_DIR_PATH+\"/weights\"\n","GDRIVE_CFG_DIR_PATH = GDRIVE_APP_BASE_DIR_PATH+\"/cfg\"\n","GITHUB_CODEBASE_DIR_PATH = \"/content/train_yolo_with_custom_dataset_on_colab_101\"\n","\n","GDRIVE_DARKNET_BIN_FILE_PATH = GITHUB_CODEBASE_DIR_PATH+\"/darknet\"\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"8T8AKZiy1zep","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":140},"executionInfo":{"status":"ok","timestamp":1592747179416,"user_tz":-480,"elapsed":84001,"user":{"displayName":"Gwo-Chuan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivghmngODeLxLuKZvFG4vSDW8-dveLXUDjGG11=s64","userId":"14590907014150515187"}},"outputId":"81a6a26e-d17d-43ed-f588-a63b6eeebb2b"},"source":["# load sample codes\n","%cd /content\n","!git clone https://github.com/wallat/train_yolo_with_custom_dataset_on_colab_101.git\n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content\n","Cloning into 'train_yolo_with_custom_dataset_on_colab_101'...\n","remote: Enumerating objects: 23, done.\u001b[K\n","remote: Counting objects: 100% (23/23), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 23 (delta 4), reused 19 (delta 3), pack-reused 0\u001b[K\n","Unpacking objects: 100% (23/23), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hiDkR-bmmSOm","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592747180648,"user_tz":-480,"elapsed":85211,"user":{"displayName":"Gwo-Chuan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivghmngODeLxLuKZvFG4vSDW8-dveLXUDjGG11=s64","userId":"14590907014150515187"}}},"source":["# build the link to avoiding type in long path name everytime\n","!ln -fs {GDRIVE_APP_BASE_DIR_REMOTE_PATH} {GDRIVE_APP_BASE_DIR_PATH}"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yc2Nj_7rB66Z","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592747180651,"user_tz":-480,"elapsed":85199,"user":{"displayName":"Gwo-Chuan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivghmngODeLxLuKZvFG4vSDW8-dveLXUDjGG11=s64","userId":"14590907014150515187"}}},"source":["# clean folders\n","\n","import os\n","import shutil\n","\n","shutil.rmtree(LOCAL_CFG_DIR_PATH, ignore_errors=True)\n","shutil.rmtree(LOCAL_YOLOS_DIR_PATH, ignore_errors=True)\n","\n","os.makedirs(GDRIVE_APP_BASE_DIR_REMOTE_PATH.replace(\"\\ \", \" \"), exist_ok=True)\n","os.makedirs(GDRIVE_CFG_DIR_PATH, exist_ok=True)\n","os.makedirs(GDRIVE_WEIGHTS_DIR_PATH, exist_ok=True)\n","\n","os.makedirs(LOCAL_CFG_DIR_PATH, exist_ok=True)\n","os.makedirs(LOCAL_YOLOS_DIR_PATH, exist_ok=True)\n","# os.makedirs(GDRIVE_CFG_DIR_PATH, exist_ok=True)\n","\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"gdAvfen4ltuX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592747180652,"user_tz":-480,"elapsed":85180,"user":{"displayName":"Gwo-Chuan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivghmngODeLxLuKZvFG4vSDW8-dveLXUDjGG11=s64","userId":"14590907014150515187"}}},"source":[""],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h4I8XuV0mfxG","colab_type":"text"},"source":["## 擷取出所有的標籤名稱\n","\n","由於 darknet 框架會將物體名字全部轉成數字，我們需要先將物體名字全部擷取出來存在一份檔案中，當作之後的對照表。"]},{"cell_type":"code","metadata":{"id":"d6vyhMZEOybf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592747181687,"user_tz":-480,"elapsed":86198,"user":{"displayName":"Gwo-Chuan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivghmngODeLxLuKZvFG4vSDW8-dveLXUDjGG11=s64","userId":"14590907014150515187"}},"outputId":"f8b0d371-a873-4cc8-feba-559c3e3fdac1"},"source":["# Convert VOC xmls into Yolo's format\n","\n","import glob\n","import os\n","import re\n","\n","labels = set()\n","for path in glob.glob(os.path.join(LOCAL_LABELS_DIR_PATH, \"*.xml\")):\n","    with open(path, 'r') as f:\n","        content = f.read()\n","\n","    # extract label names\n","    matches = re.findall(r'<name>([\\w_]+)<\\/name>', content, flags=0)\n","    labels.update(matches)\n","\n","# write label into file`\n","with open(os.path.join(GDRIVE_CFG_DIR_PATH, \"obj.names\"), 'w') as f:\n","    f.write(\"\\n\".join(labels))\n","\n","print('Read in %d labels: %s' % (len(labels), \", \".join(labels)))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Read in 2 labels: cat, dog\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_uUZXq-Lzzkk","colab_type":"text"},"source":["## 將 VOC 格式的標記資料轉成 YOLO 的標記資料\n","\n","Yolo 不是使用標準的格式，原本的 VOC 標記格式需要轉換後才能使用在 darkent 框架上。\n","這邊就不詳細解釋如何轉換，對如何轉換的詳細規格可以參考 Yolo 官網 。我們直接使用我從 convert2Yolo 套件中擷取出來的片段程式碼來執行轉換，並把轉換的結果都放到 `/content/pet_detection/yolos` 目錄中。"]},{"cell_type":"code","metadata":{"id":"jiGMipcozWPa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1592747185701,"user_tz":-480,"elapsed":90176,"user":{"displayName":"Gwo-Chuan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivghmngODeLxLuKZvFG4vSDW8-dveLXUDjGG11=s64","userId":"14590907014150515187"}},"outputId":"f0007a89-55f5-4048-86ee-3dfd67a63a1c"},"source":["import sys\n","sys.path.append(GITHUB_CODEBASE_DIR_PATH)\n","\n","from Format import VOC, YOLO\n","\n","voc = VOC()\n","yolo = YOLO(os.path.join(GDRIVE_CFG_DIR_PATH, \"obj.names\"))\n","\n","flag, data = voc.parse(LOCAL_LABELS_DIR_PATH)\n","flag, data = yolo.generate(data)\n","\n","flag, data = yolo.save(data,\n","    save_path=LOCAL_YOLOS_DIR_PATH,\n","    img_path=LOCAL_IMAGES_DIR_PATH, img_type=\".jpg\", manipast_path=\"./\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["l ['cat', 'dog']\n","\n","VOC Parsing:   |████████████████████████████████████████| 100.0% (3686/3686)  Complete\n","\n","\n","YOLO Generating:|████████████████████████████████████████| 100.0% (3686/3686)  Complete\n","\n","\n","YOLO Saving:   |████████████████████████████████████████| 100.0% (3686/3686)  Complete\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wMfMvmN367pF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592747189186,"user_tz":-480,"elapsed":93637,"user":{"displayName":"Gwo-Chuan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivghmngODeLxLuKZvFG4vSDW8-dveLXUDjGG11=s64","userId":"14590907014150515187"}}},"source":["# copy images into yolos folder\n","# !find $LOCAL_IMAGES_DIR_PATH -name \"*.jpg\" -exec cp {} /content/pet_detection/yolos \\;\n","\n","# from distutils.dir_util import copy_tree\n","# copy_tree(LOCAL_IMAGES_DIR_PATH, LOCAL_YOLOS_DIR_PATH)\n","\n","!cp {LOCAL_IMAGES_DIR_PATH}/*.jpg {LOCAL_YOLOS_DIR_PATH}"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I4FhRWwuSfbY","colab_type":"text"},"source":["## 準備訓練用的設定檔\n","\n","* `obj.names`：所有的物體標籤名稱，每一行一個。例如此例中只會有兩行，分別是 cat dog 。\n","* `yolov3.cfg`：darknet 網路的設定檔，描述每一層網路應該要如何建立，以及建立多少 node 等。裡面有些數值需要根據你的訓練資料來個別設定。\n","* `train.txt` `test.txt` ：這兩個檔案告訴 darknet 要到哪個路徑下找到訓練用的圖片。\n","* `obj.data`：darknet 的主要設定檔案，告訴 darknet 其他的設定檔路徑。darknet 會一一去讀取其他的檔案。"]},{"cell_type":"code","metadata":{"id":"wyAmNKZb_yM1","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592747190603,"user_tz":-480,"elapsed":95037,"user":{"displayName":"Gwo-Chuan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivghmngODeLxLuKZvFG4vSDW8-dveLXUDjGG11=s64","userId":"14590907014150515187"}}},"source":["# create the cfg file\n","!cp {GITHUB_CODEBASE_DIR_PATH}/darknet_cfg/yolov3.cfg {GDRIVE_CFG_DIR_PATH}/yolov3.cfg\n","\n","# fetch label_names\n","with open(os.path.join(GDRIVE_CFG_DIR_PATH, \"obj.names\"), 'r') as f:\n","  f_content = f.read()\n","label_names = f_content.strip().splitlines()\n","\n","# update the cfg file\n","with open(os.path.join(GDRIVE_CFG_DIR_PATH, \"yolov3.cfg\"), 'r') as f:\n","  content = f.read()\n","with open(os.path.join(GDRIVE_CFG_DIR_PATH, \"yolov3.cfg\"), 'w') as f:\n","  num_max_batches = len(label_names)*2000\n","  content = content.replace(\"%NUM_CLASSES%\", str(len(label_names)))\n","  content = content.replace(\"%NUM_MAX_BATCHES%\", str(num_max_batches))\n","  content = content.replace(\"%NUM_MAX_BATCHES_80%\", str(int(num_max_batches*0.8)))\n","  content = content.replace(\"%NUM_MAX_BATCHES_90%\", str(int(num_max_batches*0.9)))\n","  content = content.replace(\"%NUM_CONVOLUTIONAL_FILTERS%\", str((len(label_names)+5)*3))\n","\n","  f.write(content)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"0cb7KN85Iccf","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592747191525,"user_tz":-480,"elapsed":95944,"user":{"displayName":"Gwo-Chuan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivghmngODeLxLuKZvFG4vSDW8-dveLXUDjGG11=s64","userId":"14590907014150515187"}}},"source":["# create train and test files\n","import random\n","import glob\n","\n","txt_paths = glob.glob(os.path.join(LOCAL_YOLOS_DIR_PATH, \"*.txt\"))\n","\n","random.shuffle(txt_paths)\n","num_train_images = int(len(txt_paths)*0.8)\n","\n","assert num_train_images>0, \"There's no training images in folder %s\" % (LOCAL_YOLOS_DIR_PATH)\n","\n","with open(os.path.join(GDRIVE_CFG_DIR_PATH, \"train.txt\"), 'w') as f:\n","  for path in txt_paths[:num_train_images]:\n","    f.write(\"%s/%s\\n\" % (LOCAL_YOLOS_DIR_PATH, os.path.basename(path).replace(\".txt\", \".jpg\")))\n","with open(os.path.join(GDRIVE_CFG_DIR_PATH, \"test.txt\"), 'w') as f:\n","  for path in txt_paths[num_train_images:]:\n","    f.write(\"%s/%s\\n\" % (LOCAL_YOLOS_DIR_PATH, os.path.basename(path).replace(\".txt\", \".jpg\")))\n","\n","# create obj\n","with open(os.path.join(GDRIVE_CFG_DIR_PATH, \"obj.data\"), 'w') as f:\n","  f.write(\"classes=%d\\n\" % (len(label_names)))\n","  f.write(\"train=%s/train.txt\\n\" % (GDRIVE_CFG_DIR_PATH))\n","  f.write(\"valid=%s/test.txt\\n\" % (GDRIVE_CFG_DIR_PATH))\n","  f.write(\"names=%s/obj.names\\n\" % (GDRIVE_CFG_DIR_PATH))\n","  f.write(\"backup=%s\\n\" % (GDRIVE_WEIGHTS_DIR_PATH))"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4KLA9fOHSvAg","colab_type":"text"},"source":["## 準備 darkent 執行檔\n","\n","我們直接從之前已經編譯好的檔案複製過來就好，不用每次都重頭編譯那實在是太~花~時~間~了~。編譯的方法請見 「[如何在 Colab 安裝 Darknet 框架訓練 YOLO v3 物件辨識並且最佳化 Colab 的訓練流程](http://bit.ly/33XjcEu)」這邊文章。"]},{"cell_type":"code","metadata":{"id":"Hl6yvB81G7AT","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592747194005,"user_tz":-480,"elapsed":98404,"user":{"displayName":"Gwo-Chuan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivghmngODeLxLuKZvFG4vSDW8-dveLXUDjGG11=s64","userId":"14590907014150515187"}}},"source":["# copy the pretrained darknet bin file\n","!cp {GDRIVE_DARKNET_BIN_FILE_PATH} /content/\n","assert os.path.isfile(\"/content/darknet\"), 'Cannot copy from %s to /content' % (GDRIVE_DARKNET_BIN_FILE_PATH)\n","\n","!chmod +x /content/darknet"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NtKSu2kRTFfG","colab_type":"text"},"source":["## （可選）使用 darknet 預先訓練的基底模型\n","\n","Darknet 也好心的提供了預先訓練的模型，以此為基底，可以讓後來的訓練比較快達到較好的辨識率。但前提是你的圖片都是常見的圖片，例如一般照片、場景照片等；如果是一些遊戲畫面很少見的，從 0 開始訓練可能會達到比較好的效果。"]},{"cell_type":"code","metadata":{"id":"znh4uODyAYnU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"status":"ok","timestamp":1592747450000,"user_tz":-480,"elapsed":354385,"user":{"displayName":"Gwo-Chuan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GivghmngODeLxLuKZvFG4vSDW8-dveLXUDjGG11=s64","userId":"14590907014150515187"}},"outputId":"0f4e0522-078c-4234-8230-e54607cab42b"},"source":["# Use the pre-trained weights to speed up the training speed\n","!wget https://pjreddie.com/media/files/darknet53.conv.74"],"execution_count":13,"outputs":[{"output_type":"stream","text":["--2020-06-21 13:46:35--  https://pjreddie.com/media/files/darknet53.conv.74\n","Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n","Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 162482580 (155M) [application/octet-stream]\n","Saving to: ‘darknet53.conv.74’\n","\n","darknet53.conv.74   100%[===================>] 154.96M   545KB/s    in 4m 14s  \n","\n","2020-06-21 13:50:50 (625 KB/s) - ‘darknet53.conv.74’ saved [162482580/162482580]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"93nCcBTEUM3r","colab_type":"text"},"source":["# 終於可以開始訓練模型了～\n","\n","上面的步驟看起來很多，但其實只要寫好一次，之後每次訓練時只要換上自己要的資料夾，然後按 `Run all` 就可以了。\n","\n","這邊提供兩條指令：\n","\n","1. 是從 YOLO Pre-trained model 開始訓練，如果你的資料集是一般照片，那用這個效果會比較好。\n","\n","2. 是從上次訓練的中斷點繼續往下訓練；指令會讀取資料夾內的 weight 檔案，從上次中斷點繼續訓練下去。適合從 0 開始訓練或是被 Colab 中斷後重新開始訓練。"]},{"cell_type":"code","metadata":{"id":"aOjLLa-DGDmv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"da21e50f-e64b-4fde-ff0b-e262dce72459"},"source":["# train the model\n","\n","# !./darknet detector train {GDRIVE_CFG_DIR_PATH}/obj.data {GDRIVE_CFG_DIR_PATH}/yolov3.cfg darknet53.conv.74 -dont_show\n","!./darknet detector train {GDRIVE_CFG_DIR_PATH}/obj.data {GDRIVE_CFG_DIR_PATH}/yolov3.cfg darknet53.conv.74  -dont_show | grep \"avg loss\"\n","# !./darknet detector train {GDRIVE_CFG_DIR_PATH}/obj.data {GDRIVE_CFG_DIR_PATH}/yolov3.cfg {GDRIVE_WEIGHTS_DIR_PATH}/yolov3_last.weights  -dont_show | grep \"avg loss\"\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["layer     filters    size              input                output\n","   0 conv     32  3 x 3 / 1   416 x 416 x   3   ->   416 x 416 x  32 0.299 BF\n","   1 conv     64  3 x 3 / 2   416 x 416 x  32   ->   208 x 208 x  64 1.595 BF\n","   2 conv     32  1 x 1 / 1   208 x 208 x  64   ->   208 x 208 x  32 0.177 BF\n","   3 conv     64  3 x 3 / 1   208 x 208 x  32   ->   208 x 208 x  64 1.595 BF\n","   4 Shortcut Layer: 1\n","   5 conv    128  3 x 3 / 2   208 x 208 x  64   ->   104 x 104 x 128 1.595 BF\n","   6 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64 0.177 BF\n","   7 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128 1.595 BF\n","   8 Shortcut Layer: 5\n","   9 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64 0.177 BF\n","  10 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128 1.595 BF\n","  11 Shortcut Layer: 8\n","  12 conv    256  3 x 3 / 2   104 x 104 x 128   ->    52 x  52 x 256 1.595 BF\n","  13 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n","  14 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n","  15 Shortcut Layer: 12\n","  16 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n","  17 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n","  18 Shortcut Layer: 15\n","  19 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n","  20 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n","  21 Shortcut Layer: 18\n","  22 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n","  23 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n","  24 Shortcut Layer: 21\n","  25 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n","  26 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n","  27 Shortcut Layer: 24\n","  28 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n","  29 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n","  30 Shortcut Layer: 27\n","  31 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n","  32 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n","  33 Shortcut Layer: 30\n","  34 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n","  35 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n","  36 Shortcut Layer: 33\n","  37 conv    512  3 x 3 / 2    52 x  52 x 256   ->    26 x  26 x 512 1.595 BF\n","  38 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n","  39 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n","  40 Shortcut Layer: 37\n","  41 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n","  42 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n","  43 Shortcut Layer: 40\n","  44 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n","  45 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n","  46 Shortcut Layer: 43\n","  47 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n","  48 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n","  49 Shortcut Layer: 46\n","  50 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n","  51 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n","  52 Shortcut Layer: 49\n","  53 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n","  54 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n","  55 Shortcut Layer: 52\n","  56 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n","  57 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n","  58 Shortcut Layer: 55\n","  59 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n","  60 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n","  61 Shortcut Layer: 58\n","  62 conv   1024  3 x 3 / 2    26 x  26 x 512   ->    13 x  13 x1024 1.595 BF\n","  63 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n","  64 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n","  65 Shortcut Layer: 62\n","  66 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n","  67 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n","  68 Shortcut Layer: 65\n","  69 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n","  70 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n","  71 Shortcut Layer: 68\n","  72 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n","  73 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n","  74 Shortcut Layer: 71\n","  75 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n","  76 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n","  77 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n","  78 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n","  79 conv    512  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x 512 0.177 BF\n","  80 conv   1024  3 x 3 / 1    13 x  13 x 512   ->    13 x  13 x1024 1.595 BF\n","  81 conv     21  1 x 1 / 1    13 x  13 x1024   ->    13 x  13 x  21 0.007 BF\n","  82 yolo\n","  83 route  79\n","  84 conv    256  1 x 1 / 1    13 x  13 x 512   ->    13 x  13 x 256 0.044 BF\n","  85 upsample            2x    13 x  13 x 256   ->    26 x  26 x 256\n","  86 route  85 61\n","  87 conv    256  1 x 1 / 1    26 x  26 x 768   ->    26 x  26 x 256 0.266 BF\n","  88 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n","  89 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n","  90 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n","  91 conv    256  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x 256 0.177 BF\n","  92 conv    512  3 x 3 / 1    26 x  26 x 256   ->    26 x  26 x 512 1.595 BF\n","  93 conv     21  1 x 1 / 1    26 x  26 x 512   ->    26 x  26 x  21 0.015 BF\n","  94 yolo\n","  95 route  91\n","  96 conv    128  1 x 1 / 1    26 x  26 x 256   ->    26 x  26 x 128 0.044 BF\n","  97 upsample            2x    26 x  26 x 128   ->    52 x  52 x 128\n","  98 route  97 36\n","  99 conv    128  1 x 1 / 1    52 x  52 x 384   ->    52 x  52 x 128 0.266 BF\n"," 100 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n"," 101 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n"," 102 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n"," 103 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128 0.177 BF\n"," 104 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256 1.595 BF\n"," 105 conv     21  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x  21 0.029 BF\n"," 106 yolo\n","Total BFLOPS 65.297 \n"," Allocate additional workspace_size = 12.46 MB \n","Loading weights from darknet53.conv.74...Done!\n"," 1: 1904.732178, 1904.732178 avg loss, 0.000000 rate, 21.449855 seconds, 64 images\n"," 2: 1904.856079, 1904.744629 avg loss, 0.000000 rate, 22.421237 seconds, 128 images\n"," 3: 1905.296875, 1904.799805 avg loss, 0.000000 rate, 22.648264 seconds, 192 images\n"," 4: 1903.884766, 1904.708252 avg loss, 0.000000 rate, 22.634792 seconds, 256 images\n","Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"," 5: 1904.719604, 1904.709351 avg loss, 0.000000 rate, 22.633391 seconds, 320 images\n"," 6: 1905.348999, 1904.773315 avg loss, 0.000000 rate, 22.601648 seconds, 384 images\n"," 7: 1904.431030, 1904.739136 avg loss, 0.000000 rate, 22.623592 seconds, 448 images\n"," 8: 1904.828369, 1904.748047 avg loss, 0.000000 rate, 22.625293 seconds, 512 images\n"," 9: 1904.874634, 1904.760742 avg loss, 0.000000 rate, 22.676923 seconds, 576 images\n"," 10: 1904.406616, 1904.725342 avg loss, 0.000000 rate, 22.657724 seconds, 640 images\n"," 11: 892.694885, 1803.522339 avg loss, 0.000000 rate, 10.857006 seconds, 704 images\n"," 12: 892.923828, 1712.462524 avg loss, 0.000000 rate, 11.321049 seconds, 768 images\n","Corrupt JPEG data: 240 extraneous bytes before marker 0xd9\n"," 13: 892.971680, 1630.513428 avg loss, 0.000000 rate, 11.307192 seconds, 832 images\n"," 14: 892.511475, 1556.713257 avg loss, 0.000000 rate, 11.315873 seconds, 896 images\n"," 15: 892.813843, 1490.323364 avg loss, 0.000000 rate, 11.347825 seconds, 960 images\n"," 16: 893.129028, 1430.603882 avg loss, 0.000000 rate, 11.281894 seconds, 1024 images\n"," 17: 892.398682, 1376.783325 avg loss, 0.000000 rate, 11.323952 seconds, 1088 images\n"," 18: 892.509827, 1328.355957 avg loss, 0.000000 rate, 11.311797 seconds, 1152 images\n"," 19: 892.622864, 1284.782593 avg loss, 0.000000 rate, 11.300099 seconds, 1216 images\n"," 20: 892.425415, 1245.546875 avg loss, 0.000000 rate, 11.317996 seconds, 1280 images\n"," 21: 529.406372, 1173.932861 avg loss, 0.000000 rate, 6.647600 seconds, 1344 images\n"," 22: 528.937866, 1109.433350 avg loss, 0.000000 rate, 6.920220 seconds, 1408 images\n"," 23: 528.797119, 1051.369751 avg loss, 0.000000 rate, 6.917153 seconds, 1472 images\n"," 24: 528.846313, 999.117432 avg loss, 0.000000 rate, 6.934822 seconds, 1536 images\n"," 25: 528.883423, 952.094055 avg loss, 0.000000 rate, 6.929004 seconds, 1600 images\n"," 26: 528.911926, 909.775818 avg loss, 0.000000 rate, 6.900307 seconds, 1664 images\n"," 27: 528.819702, 871.680176 avg loss, 0.000000 rate, 6.917612 seconds, 1728 images\n"," 28: 528.734314, 837.385620 avg loss, 0.000000 rate, 6.910017 seconds, 1792 images\n"," 29: 528.718994, 806.518982 avg loss, 0.000000 rate, 6.898582 seconds, 1856 images\n"," 30: 528.080078, 778.675110 avg loss, 0.000000 rate, 6.930542 seconds, 1920 images\n"," 31: 759.705078, 776.778137 avg loss, 0.000000 rate, 9.304233 seconds, 1984 images\n"," 32: 759.449341, 775.045288 avg loss, 0.000000 rate, 9.682584 seconds, 2048 images\n"," 33: 759.674500, 773.508179 avg loss, 0.000000 rate, 9.706715 seconds, 2112 images\n"," 34: 759.475342, 772.104919 avg loss, 0.000000 rate, 9.672490 seconds, 2176 images\n"," 35: 758.374451, 770.731873 avg loss, 0.000000 rate, 9.696536 seconds, 2240 images\n"," 36: 758.774475, 769.536133 avg loss, 0.000000 rate, 9.683922 seconds, 2304 images\n"," 37: 757.998901, 768.382385 avg loss, 0.000000 rate, 9.694760 seconds, 2368 images\n"," 38: 757.665283, 767.310669 avg loss, 0.000000 rate, 9.681692 seconds, 2432 images\n"," 39: 757.408264, 766.320435 avg loss, 0.000000 rate, 9.658359 seconds, 2496 images\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xcj577W0iCoR","colab_type":"text"},"source":["# 檢視訓練成果\n","\n","檢測圖片時，我們只需要 `obj.names`, `yolov3.cfg` 以及 weights 檔就夠。我們可以直接利用 Opencv 內建的 darknet 來讀取網路並產生出預測。"]},{"cell_type":"code","metadata":{"id":"fwdKXMy5ip7i","colab_type":"code","colab":{}},"source":["TO_DETECTING_IMAGE_DIR_PATH = GITHUB_CODEBASE_DIR_PATH+\"/to_detect_images\"\n","\n","# Use python to read \n","\n","import os\n","import cv2\n","import numpy as np\n","import glob\n","from google.colab.patches import cv2_imshow\n","\n","import pprint\n","pp = pprint.PrettyPrinter(indent=4)\n","\n","\n","def detecting_one_image(net, output_layers, img):\n","  # Detecting objects\n","  # cv::dnn::blobFromImage (InputArray image, double scalefactor=1.0, const Size &size=Size(), const Scalar &mean=Scalar(), bool swapRB=false, bool crop=false, int ddepth=CV_32F)\n","  blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n","  net.setInput(blob)\n","  outs = net.forward(output_layers)\n","\n","  return outs\n","\n","# Load Yolo\n","net = cv2.dnn.readNet(GDRIVE_WEIGHTS_DIR_PATH+\"/yolov3_last.weights\", GDRIVE_CFG_DIR_PATH+\"/yolov3.cfg\")\n","layer_names = net.getLayerNames()\n","output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n","\n","# Load label names\n","with open(GDRIVE_CFG_DIR_PATH+\"/obj.names\", \"r\") as f:\n","  classes = [line.strip() for line in f.readlines()]\n","\n","# Generate display colors\n","colors = np.random.uniform(0, 255, size=(len(classes), 3))\n","\n","for fpath in glob.glob(os.path.join(TO_DETECTING_IMAGE_DIR_PATH, \"*.jpg\")):\n","  print(\"fpath\", fpath)\n","\n","  # Loading image\n","  img = cv2.imread(fpath)\n","  height, width, channels = img.shape\n","\n","  if width>800: # resize for display purpose\n","    dim = (800, int(800*height/width))\n","    img = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n","    height, width, channels = img.shape\n","\n","  outs = detecting_one_image(net, output_layers, img)\n","\n","  # Showing informations on the screen\n","  for out in outs:\n","    for detection in out:\n","      scores = detection[5:]\n","      class_id = np.argmax(scores)\n","      confidence = scores[class_id]\n","      if confidence > 0.3:\n","        # Object detected\n","        center_x = int(detection[0] * width)\n","        center_y = int(detection[1] * height)\n","        w = int(detection[2] * width)\n","        h = int(detection[3] * height)\n","\n","        # Rectangle coordinates\n","        x = int(center_x - w / 2)\n","        y = int(center_y - h / 2)\n","\n","        label = \"(%.2f) %s\" % (confidence, classes[class_id])\n","\n","        cv2.rectangle(img, (x, y), (x + w, y + h), colors[class_id], 2)\n","        cv2.putText(img, label, (x, y+h-5), cv2.FONT_HERSHEY_PLAIN, 1, colors[class_id], 1)\n","\n","  cv2_imshow(img)"],"execution_count":null,"outputs":[]}]}